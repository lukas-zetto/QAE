{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f593741a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows: 551\n",
      "Ground truth anomalous windows: 55\n",
      "\n",
      "\n",
      "=== Evaluating file results/SMD1\\ensemble_res_1.pkl (Run 1) ===\n",
      "\n",
      "=== Evaluating file results/SMD1\\ensemble_res_10.pkl (Run 10) ===\n",
      "\n",
      "=== Evaluating file results/SMD1\\ensemble_res_2.pkl (Run 2) ===\n",
      "\n",
      "=== Evaluating file results/SMD1\\ensemble_res_3.pkl (Run 3) ===\n",
      "\n",
      "=== Evaluating file results/SMD1\\ensemble_res_4.pkl (Run 4) ===\n",
      "\n",
      "=== Evaluating file results/SMD1\\ensemble_res_5.pkl (Run 5) ===\n",
      "\n",
      "=== Evaluating file results/SMD1\\ensemble_res_6.pkl (Run 6) ===\n",
      "\n",
      "=== Evaluating file results/SMD1\\ensemble_res_7.pkl (Run 7) ===\n",
      "\n",
      "=== Evaluating file results/SMD1\\ensemble_res_8.pkl (Run 8) ===\n",
      "\n",
      "=== Evaluating file results/SMD1\\ensemble_res_9.pkl (Run 9) ===\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# --- Settings for SMD ---\n",
    "window_size = 100     # SMD window size\n",
    "stride = 50           # SMD stride\n",
    "Ttotal = 27600        # total length in samples (longer than last anomaly)\n",
    "\n",
    "# --- Anomalous ranges (ground truth for SMD) ---\n",
    "anomalies = [\n",
    "    (16964, 17515),\n",
    "    (18072, 18528),\n",
    "    (19368, 20088),\n",
    "    (20787, 21195),\n",
    "    (24680, 24682),\n",
    "    (26115, 26116),\n",
    "    (27555, 27556),\n",
    "]\n",
    "\n",
    "total_windows = (Ttotal - window_size) // stride + 1\n",
    "\n",
    "# --- Ground truth anomalous windows ---\n",
    "start_times = [i * stride for i in range(total_windows)]\n",
    "anomalous_indices = set()\n",
    "for idx, start in enumerate(start_times):\n",
    "    end = start + window_size\n",
    "    for Astart, Aend in anomalies:\n",
    "        if min(end, Aend) > max(start, Astart):  # overlap check\n",
    "            anomalous_indices.add(idx)\n",
    "\n",
    "print(f\"Total windows: {total_windows}\")\n",
    "print(f\"Ground truth anomalous windows: {len(anomalous_indices)}\\n\")\n",
    "\n",
    "# --- Loop over all result files automatically ---\n",
    "result_files = glob.glob(\"results/SMD1/ensemble_res_*.pkl\")\n",
    "\n",
    "for file in result_files:\n",
    "    x = os.path.splitext(os.path.basename(file))[0].split(\"_\")[-1]\n",
    "\n",
    "    print(f\"\\n=== Evaluating file {file} (Run {x}) ===\")\n",
    "\n",
    "    with open(file, \"rb\") as f:\n",
    "        all_results = pickle.load(f)\n",
    "\n",
    "    # --- Compute anomaly scores ---\n",
    "    anomaly_scores = defaultdict(lambda: {'score_sum': 0.0, 'count': 0})\n",
    "\n",
    "    for iteration_result in all_results:\n",
    "        buckets = iteration_result['buckets']\n",
    "        bucket_results = iteration_result['bucket_results']\n",
    "\n",
    "        for bucket_result in bucket_results:\n",
    "            bucket_idx = bucket_result['bucket_idx']\n",
    "            final_results = bucket_result['final_results']\n",
    "            indices_in_bucket = buckets[bucket_idx]\n",
    "\n",
    "            mean = np.mean(final_results)\n",
    "            std = np.std(final_results) if np.std(final_results) != 0 else 1e-8\n",
    "\n",
    "            for i, idx in enumerate(indices_in_bucket):\n",
    "                sim = final_results[i]\n",
    "                deviation = abs(sim - mean) / std\n",
    "                anomaly_scores[idx]['score_sum'] += deviation\n",
    "                anomaly_scores[idx]['count'] += 1\n",
    "\n",
    "    final_scores = {\n",
    "        idx: score_data['score_sum'] / score_data['count']\n",
    "        for idx, score_data in anomaly_scores.items()\n",
    "    }\n",
    "\n",
    "    # --- Percentile threshold based on anomaly ratio ---\n",
    "    true_anomaly_count = len(anomalous_indices)\n",
    "    PERCENTILE = 100 - (true_anomaly_count / total_windows) * 100\n",
    "    all_score_values = list(final_scores.values())\n",
    "    threshold = np.percentile(all_score_values, PERCENTILE)\n",
    "\n",
    "    # --- Detected anomalous windows ---\n",
    "    detected_windows = {\n",
    "        idx: score for idx, score in final_scores.items() if score >= threshold\n",
    "    }\n",
    "    detected_indices = set(detected_windows.keys())\n",
    "\n",
    "\n",
    "    # --- Metrics ---\n",
    "    true_positives = len(detected_indices & anomalous_indices)\n",
    "    false_positives = len(detected_indices - anomalous_indices)\n",
    "    false_negatives = len(anomalous_indices - detected_indices)\n",
    "\n",
    "    true_negatives = total_windows - (true_positives + false_positives + false_negatives)\n",
    "\n",
    "    precision = true_positives / len(detected_indices) * 100 if detected_indices else 0\n",
    "    recall = true_positives / len(anomalous_indices) * 100 if anomalous_indices else 0\n",
    "    f1_score = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0\n",
    "\n",
    "    accuracy = (true_positives + true_negatives) / total_windows * 100\n",
    "\n",
    "    # Balanced Accuracy\n",
    "    tpr = recall / 100  # already in percent, divide by 100\n",
    "    tnr = true_negatives / (true_negatives + false_positives) if (true_negatives + false_positives) > 0 else 0\n",
    "    balanced_accuracy = (tpr + tnr) / 2 * 100\n",
    "\n",
    "    # AUC (needs raw scores, not thresholded)\n",
    "    labels = [1 if i in anomalous_indices else 0 for i in range(total_windows)]\n",
    "    scores = [final_scores.get(i, 0) for i in range(total_windows)]\n",
    "    auc = roc_auc_score(labels, scores) * 100\n",
    "\n",
    "\n",
    "    # --- Save results ---\n",
    "    output_dir = \"evaluations/SMD1/\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_filename = f\"results_w{x}.txt\"\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for idx, score in sorted(detected_windows.items()):\n",
    "            f.write(f\"Window {idx} - Score: {score:.4f}\\n\")\n",
    "        f.write(f\"\\nTrue Positives: {true_positives}\")\n",
    "        f.write(f\"\\nFalse Positives: {false_positives}\")\n",
    "        f.write(f\"\\nFalse Negatives: {false_negatives}\")\n",
    "        f.write(f\"\\nTrue Negatives: {true_negatives}\")\n",
    "        f.write(f\"\\nPrecision: {precision:.2f}%\")\n",
    "        f.write(f\"\\nRecall: {recall:.2f}%\")\n",
    "        f.write(f\"\\nF1 Score: {f1_score:.2f}%\")\n",
    "        f.write(f\"\\nAccuracy: {accuracy:.2f}%\")\n",
    "        f.write(f\"\\nBalanced Accuracy: {balanced_accuracy:.2f}%\")\n",
    "        f.write(f\"\\nROC AUC: {auc:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33ccfe5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows: 551\n",
      "Ground truth anomalous windows: 33\n",
      "\n",
      "\n",
      "=== Evaluating file results/SMD2\\ensemble_res_1.pkl (Run 1) ===\n",
      "\n",
      "=== Evaluating file results/SMD2\\ensemble_res_10.pkl (Run 10) ===\n",
      "\n",
      "=== Evaluating file results/SMD2\\ensemble_res_2.pkl (Run 2) ===\n",
      "\n",
      "=== Evaluating file results/SMD2\\ensemble_res_3.pkl (Run 3) ===\n",
      "\n",
      "=== Evaluating file results/SMD2\\ensemble_res_4.pkl (Run 4) ===\n",
      "\n",
      "=== Evaluating file results/SMD2\\ensemble_res_5.pkl (Run 5) ===\n",
      "\n",
      "=== Evaluating file results/SMD2\\ensemble_res_6.pkl (Run 6) ===\n",
      "\n",
      "=== Evaluating file results/SMD2\\ensemble_res_7.pkl (Run 7) ===\n",
      "\n",
      "=== Evaluating file results/SMD2\\ensemble_res_8.pkl (Run 8) ===\n",
      "\n",
      "=== Evaluating file results/SMD2\\ensemble_res_9.pkl (Run 9) ===\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# --- Settings for SMD ---\n",
    "window_size = 100     # SMD window size\n",
    "stride = 50           # SMD stride\n",
    "Ttotal = 27600        # total length in samples (longer than last anomaly)\n",
    "\n",
    "# --- Anomalous ranges (ground truth for SMD) ---\n",
    "anomalies = [\n",
    "    (4630, 4688),\n",
    "    (5487, 5491),\n",
    "    (5876, 5951),\n",
    "    (15416, 15418),\n",
    "    (15541, 15605),\n",
    "    (15926, 15973),\n",
    "    (18646, 18801),\n",
    "    (20236, 20271),\n",
    "    (22265, 22336),\n",
    "    (23094, 23115),\n",
    "\n",
    "]\n",
    "\n",
    "total_windows = (Ttotal - window_size) // stride + 1\n",
    "\n",
    "# --- Ground truth anomalous windows ---\n",
    "start_times = [i * stride for i in range(total_windows)]\n",
    "anomalous_indices = set()\n",
    "for idx, start in enumerate(start_times):\n",
    "    end = start + window_size\n",
    "    for Astart, Aend in anomalies:\n",
    "        if min(end, Aend) > max(start, Astart):  # overlap check\n",
    "            anomalous_indices.add(idx)\n",
    "\n",
    "print(f\"Total windows: {total_windows}\")\n",
    "print(f\"Ground truth anomalous windows: {len(anomalous_indices)}\\n\")\n",
    "\n",
    "# --- Loop over all result files automatically ---\n",
    "result_files = glob.glob(\"results/SMD2/ensemble_res_*.pkl\")\n",
    "\n",
    "for file in result_files:\n",
    "    x = os.path.splitext(os.path.basename(file))[0].split(\"_\")[-1]\n",
    "\n",
    "    print(f\"\\n=== Evaluating file {file} (Run {x}) ===\")\n",
    "\n",
    "    with open(file, \"rb\") as f:\n",
    "        all_results = pickle.load(f)\n",
    "\n",
    "    # --- Compute anomaly scores ---\n",
    "    anomaly_scores = defaultdict(lambda: {'score_sum': 0.0, 'count': 0})\n",
    "\n",
    "    for iteration_result in all_results:\n",
    "        buckets = iteration_result['buckets']\n",
    "        bucket_results = iteration_result['bucket_results']\n",
    "\n",
    "        for bucket_result in bucket_results:\n",
    "            bucket_idx = bucket_result['bucket_idx']\n",
    "            final_results = bucket_result['final_results']\n",
    "            indices_in_bucket = buckets[bucket_idx]\n",
    "\n",
    "            mean = np.mean(final_results)\n",
    "            std = np.std(final_results) if np.std(final_results) != 0 else 1e-8\n",
    "\n",
    "            for i, idx in enumerate(indices_in_bucket):\n",
    "                sim = final_results[i]\n",
    "                deviation = abs(sim - mean) / std\n",
    "                anomaly_scores[idx]['score_sum'] += deviation\n",
    "                anomaly_scores[idx]['count'] += 1\n",
    "\n",
    "    final_scores = {\n",
    "        idx: score_data['score_sum'] / score_data['count']\n",
    "        for idx, score_data in anomaly_scores.items()\n",
    "    }\n",
    "\n",
    "    # --- Percentile threshold based on anomaly ratio ---\n",
    "    true_anomaly_count = len(anomalous_indices)\n",
    "    PERCENTILE = 100 - (true_anomaly_count / total_windows) * 100\n",
    "    all_score_values = list(final_scores.values())\n",
    "    threshold = np.percentile(all_score_values, PERCENTILE)\n",
    "\n",
    "    # --- Detected anomalous windows ---\n",
    "    detected_windows = {\n",
    "        idx: score for idx, score in final_scores.items() if score >= threshold\n",
    "    }\n",
    "    detected_indices = set(detected_windows.keys())\n",
    "\n",
    "\n",
    "    # --- Metrics ---\n",
    "    true_positives = len(detected_indices & anomalous_indices)\n",
    "    false_positives = len(detected_indices - anomalous_indices)\n",
    "    false_negatives = len(anomalous_indices - detected_indices)\n",
    "\n",
    "    true_negatives = total_windows - (true_positives + false_positives + false_negatives)\n",
    "\n",
    "    precision = true_positives / len(detected_indices) * 100 if detected_indices else 0\n",
    "    recall = true_positives / len(anomalous_indices) * 100 if anomalous_indices else 0\n",
    "    f1_score = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0\n",
    "\n",
    "    accuracy = (true_positives + true_negatives) / total_windows * 100\n",
    "\n",
    "    # Balanced Accuracy\n",
    "    tpr = recall / 100  # already in percent, divide by 100\n",
    "    tnr = true_negatives / (true_negatives + false_positives) if (true_negatives + false_positives) > 0 else 0\n",
    "    balanced_accuracy = (tpr + tnr) / 2 * 100\n",
    "\n",
    "    # AUC (needs raw scores, not thresholded)\n",
    "    labels = [1 if i in anomalous_indices else 0 for i in range(total_windows)]\n",
    "    scores = [final_scores.get(i, 0) for i in range(total_windows)]\n",
    "    auc = roc_auc_score(labels, scores) * 100\n",
    "\n",
    "\n",
    "    # --- Save results ---\n",
    "    output_dir = \"evaluations/SMD2/\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_filename = f\"results_w{x}.txt\"\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for idx, score in sorted(detected_windows.items()):\n",
    "            f.write(f\"Window {idx} - Score: {score:.4f}\\n\")\n",
    "        f.write(f\"\\nTrue Positives: {true_positives}\")\n",
    "        f.write(f\"\\nFalse Positives: {false_positives}\")\n",
    "        f.write(f\"\\nFalse Negatives: {false_negatives}\")\n",
    "        f.write(f\"\\nTrue Negatives: {true_negatives}\")\n",
    "        f.write(f\"\\nPrecision: {precision:.2f}%\")\n",
    "        f.write(f\"\\nRecall: {recall:.2f}%\")\n",
    "        f.write(f\"\\nF1 Score: {f1_score:.2f}%\")\n",
    "        f.write(f\"\\nAccuracy: {accuracy:.2f}%\")\n",
    "        f.write(f\"\\nBalanced Accuracy: {balanced_accuracy:.2f}%\")\n",
    "        f.write(f\"\\nROC AUC: {auc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52bab615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows: 551\n",
      "Ground truth anomalous windows: 55\n",
      "\n",
      "\n",
      "=== Evaluating file results/SMD_s5\\ensemble_res_1.pkl (Run 1) ===\n",
      "Threshold (99th percentile): 0.8407\n",
      "Precision: 0.49\n",
      "Recall: 0.51\n",
      "F1 Score: 0.50\n",
      "Accuracy: 0.90\n",
      "Balanced Accuracy: 0.73\n",
      "AUC: 0.87\n",
      "Percentage of anomalous windows: 9.98%\n",
      "Saved to evaluations/SMD_s5/results_w1_99th_percentile.txt\n",
      "\n",
      "=== Evaluating file results/SMD_s5\\ensemble_res_10.pkl (Run 10) ===\n",
      "Threshold (99th percentile): 0.8409\n",
      "Precision: 0.50\n",
      "Recall: 0.51\n",
      "F1 Score: 0.50\n",
      "Accuracy: 0.90\n",
      "Balanced Accuracy: 0.73\n",
      "AUC: 0.83\n",
      "Percentage of anomalous windows: 9.98%\n",
      "Saved to evaluations/SMD_s5/results_w10_99th_percentile.txt\n",
      "\n",
      "=== Evaluating file results/SMD_s5\\ensemble_res_2.pkl (Run 2) ===\n",
      "Threshold (99th percentile): 0.8392\n",
      "Precision: 0.46\n",
      "Recall: 0.47\n",
      "F1 Score: 0.46\n",
      "Accuracy: 0.89\n",
      "Balanced Accuracy: 0.71\n",
      "AUC: 0.82\n",
      "Percentage of anomalous windows: 9.98%\n",
      "Saved to evaluations/SMD_s5/results_w2_99th_percentile.txt\n",
      "\n",
      "=== Evaluating file results/SMD_s5\\ensemble_res_3.pkl (Run 3) ===\n",
      "Threshold (99th percentile): 0.8368\n",
      "Precision: 0.49\n",
      "Recall: 0.51\n",
      "F1 Score: 0.50\n",
      "Accuracy: 0.90\n",
      "Balanced Accuracy: 0.73\n",
      "AUC: 0.86\n",
      "Percentage of anomalous windows: 9.98%\n",
      "Saved to evaluations/SMD_s5/results_w3_99th_percentile.txt\n",
      "\n",
      "=== Evaluating file results/SMD_s5\\ensemble_res_4.pkl (Run 4) ===\n",
      "Threshold (99th percentile): 0.8381\n",
      "Precision: 0.53\n",
      "Recall: 0.55\n",
      "F1 Score: 0.54\n",
      "Accuracy: 0.91\n",
      "Balanced Accuracy: 0.75\n",
      "AUC: 0.86\n",
      "Percentage of anomalous windows: 9.98%\n",
      "Saved to evaluations/SMD_s5/results_w4_99th_percentile.txt\n",
      "\n",
      "=== Evaluating file results/SMD_s5\\ensemble_res_5.pkl (Run 5) ===\n",
      "Threshold (99th percentile): 0.8373\n",
      "Precision: 0.52\n",
      "Recall: 0.53\n",
      "F1 Score: 0.52\n",
      "Accuracy: 0.90\n",
      "Balanced Accuracy: 0.74\n",
      "AUC: 0.86\n",
      "Percentage of anomalous windows: 9.98%\n",
      "Saved to evaluations/SMD_s5/results_w5_99th_percentile.txt\n",
      "\n",
      "=== Evaluating file results/SMD_s5\\ensemble_res_6.pkl (Run 6) ===\n",
      "Threshold (99th percentile): 0.8412\n",
      "Precision: 0.50\n",
      "Recall: 0.51\n",
      "F1 Score: 0.50\n",
      "Accuracy: 0.90\n",
      "Balanced Accuracy: 0.73\n",
      "AUC: 0.86\n",
      "Percentage of anomalous windows: 9.98%\n",
      "Saved to evaluations/SMD_s5/results_w6_99th_percentile.txt\n",
      "\n",
      "=== Evaluating file results/SMD_s5\\ensemble_res_7.pkl (Run 7) ===\n",
      "Threshold (99th percentile): 0.8376\n",
      "Precision: 0.53\n",
      "Recall: 0.55\n",
      "F1 Score: 0.54\n",
      "Accuracy: 0.91\n",
      "Balanced Accuracy: 0.75\n",
      "AUC: 0.86\n",
      "Percentage of anomalous windows: 9.98%\n",
      "Saved to evaluations/SMD_s5/results_w7_99th_percentile.txt\n",
      "\n",
      "=== Evaluating file results/SMD_s5\\ensemble_res_8.pkl (Run 8) ===\n",
      "Threshold (99th percentile): 0.8357\n",
      "Precision: 0.51\n",
      "Recall: 0.53\n",
      "F1 Score: 0.52\n",
      "Accuracy: 0.90\n",
      "Balanced Accuracy: 0.74\n",
      "AUC: 0.88\n",
      "Percentage of anomalous windows: 9.98%\n",
      "Saved to evaluations/SMD_s5/results_w8_99th_percentile.txt\n",
      "\n",
      "=== Evaluating file results/SMD_s5\\ensemble_res_9.pkl (Run 9) ===\n",
      "Threshold (99th percentile): 0.8403\n",
      "Precision: 0.49\n",
      "Recall: 0.51\n",
      "F1 Score: 0.50\n",
      "Accuracy: 0.90\n",
      "Balanced Accuracy: 0.73\n",
      "AUC: 0.87\n",
      "Percentage of anomalous windows: 9.98%\n",
      "Saved to evaluations/SMD_s5/results_w9_99th_percentile.txt\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# --- Settings for SMD ---\n",
    "window_size = 100\n",
    "stride = 50\n",
    "Ttotal = 27600\n",
    "\n",
    "# --- Ground truth anomaly ranges ---\n",
    "anomalies = [\n",
    "    (16964, 17515),\n",
    "    (18072, 18528),\n",
    "    (19368, 20088),\n",
    "    (20787, 21195),\n",
    "    (24680, 24682),\n",
    "    (26115, 26116),\n",
    "    (27555, 27556),\n",
    "]\n",
    "\n",
    "total_windows = (Ttotal - window_size) // stride + 1\n",
    "start_times = [i * stride for i in range(total_windows)]\n",
    "anomalous_indices = set()\n",
    "for idx, start in enumerate(start_times):\n",
    "    end = start + window_size\n",
    "    for Astart, Aend in anomalies:\n",
    "        if min(end, Aend) > max(start, Astart):  # overlap check\n",
    "            anomalous_indices.add(idx)\n",
    "\n",
    "print(f\"Total windows: {total_windows}\")\n",
    "print(f\"Ground truth anomalous windows: {len(anomalous_indices)}\\n\")\n",
    "\n",
    "# --- Load results ---\n",
    "result_files = glob.glob(\"results/SMD_s5/ensemble_res_*.pkl\")\n",
    "\n",
    "for file in result_files:\n",
    "    x = os.path.splitext(os.path.basename(file))[0].split(\"_\")[-1]\n",
    "    print(f\"\\n=== Evaluating file {file} (Run {x}) ===\")\n",
    "\n",
    "    with open(file, \"rb\") as f:\n",
    "        all_results = pickle.load(f)\n",
    "\n",
    "    # --- Compute anomaly scores ---\n",
    "    anomaly_scores = defaultdict(lambda: {'score_sum': 0.0, 'count': 0})\n",
    "    for iteration_result in all_results:\n",
    "        buckets = iteration_result['buckets']\n",
    "        bucket_results = iteration_result['bucket_results']\n",
    "        for bucket_result in bucket_results:\n",
    "            bucket_idx = bucket_result['bucket_idx']\n",
    "            final_results = bucket_result['final_results']\n",
    "            indices_in_bucket = buckets[bucket_idx]\n",
    "            mean = np.mean(final_results)\n",
    "            std = np.std(final_results) if np.std(final_results) != 0 else 1e-8\n",
    "            for i, idx in enumerate(indices_in_bucket):\n",
    "                sim = final_results[i]\n",
    "                deviation = abs(sim - mean) / std\n",
    "                anomaly_scores[idx]['score_sum'] += deviation\n",
    "                anomaly_scores[idx]['count'] += 1\n",
    "\n",
    "    final_scores = {\n",
    "        idx: score_data['score_sum'] / score_data['count']\n",
    "        for idx, score_data in anomaly_scores.items()\n",
    "    }\n",
    "\n",
    "    # --- Ground truth labels (0 = normal, 1 = anomaly) ---\n",
    "    y_true = np.array([1 if idx in anomalous_indices else 0 for idx in range(total_windows)])\n",
    "    y_scores = np.array([final_scores.get(idx, 0.0) for idx in range(total_windows)])\n",
    "\n",
    "    # --- Thresholding based on the 99th percentile of scores ---\n",
    "    threshold = np.percentile(list(final_scores.values()), 90)  # Use the 99th percentile as threshold\n",
    "    y_pred = (y_scores >= threshold).astype(int)\n",
    "\n",
    "    # --- Metrics ---\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    bal_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_scores)\n",
    "\n",
    "    print(f\"Threshold (99th percentile): {threshold:.4f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Balanced Accuracy: {bal_accuracy:.2f}\")\n",
    "    print(f\"AUC: {auc:.2f}\")\n",
    "\n",
    "    # --- Save results ---\n",
    "    output_dir = \"evaluations/SMD_s5/\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_filename = f\"results_w{x}_99th_percentile.txt\"\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(f\"Threshold (99th percentile): {threshold:.4f}\\n\")\n",
    "        f.write(f\"Precision: {precision:.2f}\\n\")\n",
    "        f.write(f\"Recall: {recall:.2f}\\n\")\n",
    "        f.write(f\"F1 Score: {f1:.2f}\\n\")\n",
    "        f.write(f\"Accuracy: {accuracy:.2f}\\n\")\n",
    "        f.write(f\"Balanced Accuracy: {bal_accuracy:.2f}\\n\")\n",
    "        f.write(f\"AUC: {auc:.2f}\\n\")\n",
    "\n",
    "    anomalous_percentage = (len(anomalous_indices) / total_windows) * 100\n",
    "    print(f\"Percentage of anomalous windows: {anomalous_percentage:.2f}%\")\n",
    "\n",
    "    print(f\"Saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1341aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows: 551\n",
      "Ground truth anomalous windows: 24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# --- Settings for SMD ---\n",
    "window_size = 100\n",
    "stride = 50\n",
    "Ttotal = 23694\n",
    "\n",
    "# --- Ground truth anomaly ranges ---\n",
    "anomalies = [\n",
    "    (4630, 4688),\n",
    "    (5487, 5491),\n",
    "    (5876, 5951),\n",
    "    (15416, 15418),\n",
    "    (15541, 15605),\n",
    "    (15926, 15973),\n",
    "    (18646, 18801),\n",
    "    (20236, 20271),\n",
    "    (22265, 22336),\n",
    "    (23094, 23115),\n",
    "\n",
    "]\n",
    "\n",
    "total_windows = (Ttotal - window_size) // stride + 1\n",
    "start_times = [i * stride for i in range(total_windows)]\n",
    "anomalous_indices = set()\n",
    "for idx, start in enumerate(start_times):\n",
    "    end = start + window_size\n",
    "    for Astart, Aend in anomalies:\n",
    "        if min(end, Aend) > max(start, Astart):  # overlap check\n",
    "            anomalous_indices.add(idx)\n",
    "\n",
    "print(f\"Total windows: {total_windows}\")\n",
    "print(f\"Ground truth anomalous windows: {len(anomalous_indices)}\\n\")\n",
    "\n",
    "# --- Load results ---\n",
    "result_files = glob.glob(\"results/SMD2/ensemble_res_*.pkl\")\n",
    "\n",
    "for file in result_files:\n",
    "    x = os.path.splitext(os.path.basename(file))[0].split(\"_\")[-1]\n",
    "    print(f\"\\n=== Evaluating file {file} (Run {x}) ===\")\n",
    "\n",
    "    with open(file, \"rb\") as f:\n",
    "        all_results = pickle.load(f)\n",
    "\n",
    "    # --- Compute anomaly scores ---\n",
    "    anomaly_scores = defaultdict(lambda: {'score_sum': 0.0, 'count': 0})\n",
    "    for iteration_result in all_results:\n",
    "        buckets = iteration_result['buckets']\n",
    "        bucket_results = iteration_result['bucket_results']\n",
    "        for bucket_result in bucket_results:\n",
    "            bucket_idx = bucket_result['bucket_idx']\n",
    "            final_results = bucket_result['final_results']\n",
    "            indices_in_bucket = buckets[bucket_idx]\n",
    "            mean = np.mean(final_results)\n",
    "            std = np.std(final_results) if np.std(final_results) != 0 else 1e-8\n",
    "            for i, idx in enumerate(indices_in_bucket):\n",
    "                sim = final_results[i]\n",
    "                deviation = abs(sim - mean) / std\n",
    "                anomaly_scores[idx]['score_sum'] += deviation\n",
    "                anomaly_scores[idx]['count'] += 1\n",
    "\n",
    "    final_scores = {\n",
    "        idx: score_data['score_sum'] / score_data['count']\n",
    "        for idx, score_data in anomaly_scores.items()\n",
    "    }\n",
    "\n",
    "    # --- Ground truth labels (0 = normal, 1 = anomaly) ---\n",
    "    y_true = np.array([1 if idx in anomalous_indices else 0 for idx in range(total_windows)])\n",
    "    y_scores = np.array([final_scores.get(idx, 0.0) for idx in range(total_windows)])\n",
    "\n",
    "    # --- Thresholding based on the 99th percentile of scores ---\n",
    "    threshold = np.percentile(list(final_scores.values()), 90)  # Use the 99th percentile as threshold\n",
    "    y_pred = (y_scores >= threshold).astype(int)\n",
    "\n",
    "    # --- Metrics ---\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    bal_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_scores)\n",
    "\n",
    "    print(f\"Threshold (99th percentile): {threshold:.4f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Balanced Accuracy: {bal_accuracy:.2f}\")\n",
    "    print(f\"AUC: {auc:.2f}\")\n",
    "\n",
    "    # --- Save results ---\n",
    "    output_dir = \"evaluations/SMD2/\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_filename = f\"results_w{x}_99th_percentile.txt\"\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(f\"Threshold (99th percentile): {threshold:.4f}\\n\")\n",
    "        f.write(f\"Precision: {precision:.2f}\\n\")\n",
    "        f.write(f\"Recall: {recall:.2f}\\n\")\n",
    "        f.write(f\"F1 Score: {f1:.2f}\\n\")\n",
    "        f.write(f\"Accuracy: {accuracy:.2f}\\n\")\n",
    "        f.write(f\"Balanced Accuracy: {bal_accuracy:.2f}\\n\")\n",
    "        f.write(f\"AUC: {auc:.2f}\\n\")\n",
    "\n",
    "    anomalous_percentage = (len(anomalous_indices) / total_windows) * 100\n",
    "    print(f\"Percentage of anomalous windows: {anomalous_percentage:.2f}%\")\n",
    "\n",
    "    print(f\"Saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4fb4e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
