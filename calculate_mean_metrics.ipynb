{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4156a496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TP                    18.500\n",
       "FP                    10.500\n",
       "FN                    14.500\n",
       "TN                   507.500\n",
       "Precision             63.794\n",
       "Recall                56.065\n",
       "F1                    59.676\n",
       "Accuracy              95.461\n",
       "Balanced_Accuracy     77.016\n",
       "ROC_AUC               89.046\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your evaluation files\n",
    "folder_path = \"evaluations/SMD2\"\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "metrics_list = []\n",
    "\n",
    "# Regular expressions to extract metrics\n",
    "metric_patterns = {\n",
    "    \"TP\": r\"True Positives:\\s*(\\d+)\",\n",
    "    \"FP\": r\"False Positives:\\s*(\\d+)\",\n",
    "    \"FN\": r\"False Negatives:\\s*(\\d+)\",\n",
    "    \"TN\": r\"True Negatives:\\s*(\\d+)\",\n",
    "    \"Precision\": r\"Precision:\\s*([\\d\\.]+)%\",\n",
    "    \"Recall\": r\"Recall:\\s*([\\d\\.]+)%\",\n",
    "    \"F1\": r\"F1 Score:\\s*([\\d\\.]+)%\",\n",
    "    \"Accuracy\": r\"Accuracy:\\s*([\\d\\.]+)%\",\n",
    "    \"Balanced_Accuracy\": r\"Balanced Accuracy:\\s*([\\d\\.]+)%\",\n",
    "    \"ROC_AUC\": r\"ROC AUC:\\s*([\\d\\.]+)%\"\n",
    "}\n",
    "\n",
    "# Iterate through all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    \n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            content = f.read()\n",
    "            metrics = {}\n",
    "            for key, pattern in metric_patterns.items():\n",
    "                match = re.search(pattern, content)\n",
    "                if match:\n",
    "                    metrics[key] = float(match.group(1))\n",
    "            if metrics:\n",
    "                metrics_list.append(metrics)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_metrics = pd.DataFrame(metrics_list)\n",
    "\n",
    "# Convert percentages to fractions where needed\n",
    "percentage_columns = [\"Precision\", \"Recall\", \"F1\", \"Accuracy\", \"Balanced_Accuracy\", \"ROC_AUC\"]\n",
    "for col in percentage_columns:\n",
    "    df_metrics[col] = df_metrics[col] / 100.0\n",
    "\n",
    "# Show the dataframe\n",
    "df_metrics.head()\n",
    "\n",
    "# %%\n",
    "# Calculate mean for each metric\n",
    "mean_metrics = df_metrics.mean()\n",
    "mean_metrics_percentage = mean_metrics.copy()\n",
    "mean_metrics_percentage[percentage_columns] = mean_metrics[percentage_columns] * 100  # back to %\n",
    "mean_metrics_percentage\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
